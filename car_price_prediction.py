# -*- coding: utf-8 -*-
"""Car price prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CINDw244rJ2nimOj5A_o1Z-pLfiefiVr

Importing Dependencies
"""

import pandas as pd

"""Data Loading"""

df=pd.read_csv('/content/car data.csv')

df.head()

"""Data Exploration"""

df.isnull().sum()

df.duplicated().sum()

df = df.drop_duplicates(keep='last')

df.duplicated().sum()

df.dtypes

df.columns

"""Encoding"""

from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Create df_encoded by copying df before applying encoding
df_encoded = df.copy()

# Now you can apply the encoding to the 'Selling_type' column
df_encoded['Selling_type'] = label_encoder.fit_transform(df_encoded['Selling_type'])

# Continue with encoding other columns using df_encoded...
df['Fuel_Type'] = label_encoder.fit_transform(df['Fuel_Type'])
df['Seller_Type'] = label_encoder.fit_transform(df['Selling_type'])
df['Transmission'] = label_encoder.fit_transform(df['Transmission'])
df['Owner'] = label_encoder.fit_transform(df['Owner'])

print(df)
print(df_encoded.head())

from sklearn.preprocessing import LabelEncoder

# Ensure the column is in string format
df_encoded['Selling_type'] = df_encoded['Selling_type'].astype(str)

# Initialize and apply LabelEncoder
label_encoder = LabelEncoder()
df_encoded['Selling_type'] = label_encoder.fit_transform(df_encoded['Selling_type'])

# Convert encoded integers to float
df_encoded['Selling_type'] = df_encoded['Selling_type'].astype(float)

# Check data type
print(df_encoded.dtypes)

import pandas as pd

# Apply One-Hot Encoding for categorical columns
df_encoded = pd.get_dummies(df, columns=['Fuel_Type', 'Transmission', 'Seller_Type'])

# Apply Label Encoding for ordinal columns like 'Owner'
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
df_encoded['Owner'] = label_encoder.fit_transform(df_encoded['Owner'])

# Check the DataFrame
print(df_encoded.head())

df.head()

"""Feature Scaling"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Assuming your DataFrame is named 'df' and numerical features are in columns 'Present_Price', 'Kms_Driven', and 'Year'
numerical_features = ['Present_Price', 'Driven_kms', 'Year']

# Create a MinMaxScaler object
scaler = MinMaxScaler()

# Fit the scaler to your numerical data and transform it
df[numerical_features] = scaler.fit_transform(df[numerical_features])

# Display the scaled DataFrame
df.head()

"""Feature Selection"""

import seaborn as sns
import matplotlib.pyplot as plt

# Drop non-numeric columns before calculating correlation
numeric_df_encoded = df_encoded.select_dtypes(include=['number'])

# Plot correlation matrix
corr = numeric_df_encoded.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')
plt.show()

"""Train-Test-Split"""

from sklearn.model_selection import train_test_split

# Split the data into features (X) and target (y)
X = df_encoded.drop('Selling_Price', axis=1)  # Features (drop target column)
y = df_encoded['Selling_Price']  # Target variable

# Split into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f'Training data size: {X_train.shape[0]} rows')
print(f'Testing data size: {X_test.shape[0]} rows')

"""Model selection and training"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Assuming your DataFrame is named 'df'

# Define categorical and numerical features
categorical_features = ['Fuel_Type', 'Transmission', 'Seller_Type']
numerical_features = ['Year', 'Present_Price', 'Driven_kms', 'Owner']

# Create a ColumnTransformer for preprocessing
preprocessor = ColumnTransformer(
    transformers=[
        ('num', 'passthrough', numerical_features),
        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features),
    ])

# Define models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(random_state=42)
}

# Split the data into features (X) and target (y)
X = df[['Year', 'Present_Price', 'Driven_kms', 'Fuel_Type', 'Transmission', 'Seller_Type', 'Owner']]
y = df['Selling_Price']

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model selection and training
best_model = None
best_score = -float('inf')

for model_name, model in models.items():
    # Create a pipeline with the preprocessor and the model
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', model),
    ])

    # Fit the pipeline to the training data
    pipeline.fit(X_train, y_train)

    # Predict on the test data
    y_pred = pipeline.predict(X_test)

    # Evaluate the model
    r2 = r2_score(y_test, y_pred)

    print(f'{model_name}: R-squared = {r2:.4f}')

    if r2 > best_score:
        best_score = r2
        best_model = pipeline

print(f'\nBest Model: {best_model.named_steps["regressor"].__class__.__name__}')
print(f'Best R-squared: {best_score:.4f}')

# Now you can use the 'best_model' for further analysis or prediction

"""Hyper Tuning"""

from sklearn.model_selection import GridSearchCV

# Example of hyperparameter tuning for Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=3, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best parameters and best score
print(grid_search.best_params_)
print(grid_search.best_score_)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
from sklearn.ensemble import RandomForestRegressor # Import RandomForestRegressor


# Assuming you have the following:
# - initial_model: your initial (untuned) model
# - tuned_model: your tuned model (best model after hyperparameter tuning)
# - X_train, X_test: your feature data
# - y_train, y_test: your target variable

# 1. Evaluate the performance of the initial (untuned) model:
initial_model = RandomForestRegressor(n_estimators=100, random_state=42) # Initialize initial_model
initial_model.fit(X_train, y_train)
y_pred_initial = initial_model.predict(X_test)

# Calculate R², MAE, MSE, RMSE for the initial model:
r2_initial = r2_score(y_test, y_pred_initial)
mae_initial = mean_absolute_error(y_test, y_pred_initial)
mse_initial = mean_squared_error(y_test, y_pred_initial)
rmse_initial = np.sqrt(mse_initial)

# 2. Evaluate the performance of the tuned model:
tuned_model = grid_search.best_estimator_  # Get the best model from GridSearchCV
#tuned_model.fit(X_train, y_train)  # No need to refit, it's already fitted
y_pred_tuned = tuned_model.predict(X_test)

# Calculate R², MAE, MSE, RMSE for the tuned model:
r2_tuned = r2_score(y_test, y_pred_tuned)
mae_tuned = mean_absolute_error(y_test, y_pred_tuned)
mse_tuned = mean_squared_error(y_test, y_pred_tuned)
rmse_tuned = np.sqrt(mse_tuned)

# Print the performance comparison:
print("Initial Model Performance:")
print(f"R²: {r2_initial}")
print(f"MAE: {mae_initial}")
print(f"MSE: {mse_initial}")
print(f"RMSE: {rmse_initial}")
print(f"Accuracy of the model: {r2_initial * 100:.2f}%") # Accuracy as a percentage
print("\nTuned Model Performance:")

print(f"R²: {r2_tuned}")
print(f"MAE: {mae_tuned}")
print(f"MSE: {mse_tuned}")
print(f"RMSE: {rmse_tuned}")
print(f"Accuracy of the model: {r2_tuned * 100:.2f}%") # Accuracy as a percentage
print("\nTuned Model Performance:")

"""As we can see the accuracy of the model has increased after Hyper tuning!"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Assuming your trained model is 'tuned_model' and the new car data is 'new_car'

# Create LabelEncoders for Fuel_Type, Transmission, and Seller_Type
fuel_type_encoder = LabelEncoder().fit(df['Fuel_Type'].unique())
transmission_encoder = LabelEncoder().fit(df['Transmission'].unique())
seller_type_encoder = LabelEncoder().fit(df['Selling_type'].unique())

# Function to preprocess the new car data
def preprocess_new_car(new_car_data):
    """
    Preprocesses the new car data for prediction.

    Args:
        new_car_data (dict): Dictionary containing new car data.

    Returns:
        pandas.DataFrame: Preprocessed DataFrame ready for prediction.
    """
    new_car = new_car_data.copy()  # Create a copy to avoid modifying the original data
    new_car['Fuel_Type'] = fuel_type_encoder.transform([new_car['Fuel_Type']])[0]
    new_car['Transmission'] = transmission_encoder.transform([new_car['Transmission']])[0]
    new_car['Seller_Type'] = seller_type_encoder.transform([new_car['Seller_Type']])[0]
    new_car_df = pd.DataFrame([new_car])
    training_columns = X_train.columns  # Assuming X_train is available
    new_car_df = new_car_df.reindex(columns=training_columns, fill_value=0)
    return new_car_df

# Example new car data
new_car = {
    'Year': 2015,
    'Present_Price': 7.00,
    'Driven_kms': 35000,
    'Fuel_Type': 2,
    'Transmission': 1,
    'Seller_Type': 'Dealer',
    'Owner': 1  # First owner
}

# Preprocess the new car data
new_car_df = preprocess_new_car(new_car)

# Make the prediction
predicted_price = tuned_model.predict(new_car_df)[0]

print(f"Predicted Price for the new car: {predicted_price:.2f}")

"""Visualizations"""

import matplotlib.pyplot as plt
import seaborn as sns

# Plotting the distribution of numerical features
df['Present_Price'].hist(bins=20, edgecolor='black')
plt.title('Distribution of Present Price')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

# Plot correlation heatmap
plt.figure(figsize=(10, 6))
# Exclude non-numeric columns from correlation calculation
sns.heatmap(df.select_dtypes(include=['number']).corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

# Scatter plot for Present_Price vs Selling_Price
plt.scatter(df['Present_Price'], df['Selling_Price'])
plt.title('Present Price vs Selling Price')
plt.xlabel('Present Price')
plt.ylabel('Selling Price')
plt.show()

import matplotlib.pyplot as plt

top_10_cars = df.sort_values(by=['Selling_Price'], ascending=False).head(10)[['Car_Name', 'Selling_Price', 'Year', 'Present_Price']]  # Select desired columns
print(top_10_cars)

# Assuming your DataFrame is named 'df'
top_10_cars = df.sort_values(by=['Selling_Price'], ascending=False).head(10)

# Create a pie chart
plt.figure(figsize=(8, 8))  # Adjust figure size if needed
plt.pie(top_10_cars['Selling_Price'], labels=top_10_cars['Car_Name'], autopct='%1.1f%%', startangle=90)
plt.title('Distribution of Top 10 Highest-Priced Cars')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

import matplotlib.pyplot as plt
top_10_selling_cars = df['Car_Name'].value_counts().head(10)

# Create a pie chart
plt.figure(figsize=(8, 8))  # Adjust figure size if needed
plt.pie(top_10_selling_cars, labels=top_10_selling_cars.index, autopct='%1.1f%%', startangle=90)
plt.title('Top 10 Most Selling Car Models')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

"""# Conclusion
The Car Price Prediction project successfully demonstrated the use of machine learning to estimate the selling price of used cars based on key features like age, fuel type, transmission, and ownership. Through this project:

We performed data cleaning and preprocessing, ensuring the dataset was model-ready.

We applied feature engineering and encoding to handle categorical variables.

We trained and evaluated regression models such as Linear Regression and Random Forest.

Our best-performing model showed high accuracy, proving the features selected were strong indicators of car value.

We gained practical experience in exploratory data analysis, model training, and performance evaluation.

**Highest priced car**: Land Cruiser

**Cheapest Car**: Innova

**The most selling car model** is city.
And we can also see **Royal Enfield Classic 350 ** is not selling well.

**Average Selling Price**: 4.66

**Price Range**: 34.65

**Number of Unique Car Models**: 98

**The year with the most sales is**: 2015
"""